{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFqguKHrQEHV"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VopxXV4gN5Ph",
        "outputId": "1813d0fb-2af5-4f34-c2ba-7850c658008f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-02 15:52:57--  https://nlp.stanford.edu/data/wordvecs/glove.2024.wikigiga.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.2024.wikigiga.300d.zip [following]\n",
            "--2025-11-02 15:52:58--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.2024.wikigiga.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1705239207 (1.6G) [application/zip]\n",
            "Saving to: â€˜glove.2024.wikigiga.300d.zipâ€™\n",
            "\n",
            "glove.2024.wikigiga 100%[===================>]   1.59G  4.99MB/s    in 5m 20s  \n",
            "\n",
            "2025-11-02 15:58:18 (5.09 MB/s) - â€˜glove.2024.wikigiga.300d.zipâ€™ saved [1705239207/1705239207]\n",
            "\n",
            "--2025-11-02 15:58:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.92.189, 52.217.107.22, 52.216.44.144, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.92.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-11-02 15:58:18 ERROR 404: Not Found.\n",
            "\n",
            "--2025-11-02 15:58:18--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.240.184.75, 54.240.184.45, 54.240.184.91, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.240.184.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: â€˜wiki-news-300d-1M.vec.zipâ€™\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  29.7MB/s    in 15s     \n",
            "\n",
            "2025-11-02 15:58:33 (44.5 MB/s) - â€˜wiki-news-300d-1M.vec.zipâ€™ saved [681808098/681808098]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/wordvecs/glove.2024.wikigiga.300d.zip\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlrzhcDOSzkP",
        "outputId": "f86ebd4f-00a1-4c18-da53-3cdbe9338da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-02 16:05:21--  https://github.com/mmihaltz/word2vec-GoogleNews-vectors/raw/refs/heads/master/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/mmihaltz/word2vec-GoogleNews-vectors/refs/heads/master/GoogleNews-vectors-negative300.bin.gz [following]\n",
            "--2025-11-02 16:05:21--  https://media.githubusercontent.com/media/mmihaltz/word2vec-GoogleNews-vectors/refs/heads/master/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/octet-stream]\n",
            "Saving to: â€˜GoogleNews-vectors-negative300.bin.gzâ€™\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  87.0MB/s    in 18s     \n",
            "\n",
            "2025-11-02 16:05:39 (89.5 MB/s) - â€˜GoogleNews-vectors-negative300.bin.gzâ€™ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/mmihaltz/word2vec-GoogleNews-vectors/raw/refs/heads/master/GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It3cuBC3Rwav"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC_TWQGQSHZP",
        "outputId": "80810015-c622-4467-8dee-765babf5b3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dn8AmmFsRaJZ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "model.save_word2vec_format(\"GoogleNews-300d.txt\", binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tS3hpCvwjsQ7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "src = Path(\"/content/GoogleNews-300d.txt\")\n",
        "dst = Path(\"GoogleNews_300d.txt\")\n",
        "\n",
        "with src.open(\"r\", encoding=\"utf8\") as f_in, dst.open(\"w\", encoding=\"utf8\") as f_out:\n",
        "    next(f_in)\n",
        "    for line in f_in:\n",
        "        f_out.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wHMr9olSYre",
        "outputId": "cce4045b-1cf6-472b-9013-4770c76476f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ]
        }
      ],
      "source": [
        "!unzip wiki-news-300d-1M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0mFlk0CYSoi"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "src = Path(\"wiki-news-300d-1M.vec\")\n",
        "dst = Path(\"wiki-news-300d-1M.txt\")\n",
        "\n",
        "with src.open(\"r\", encoding=\"utf8\") as f_in, dst.open(\"w\", encoding=\"utf8\") as f_out:\n",
        "    next(f_in)\n",
        "    for line in f_in:\n",
        "        f_out.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mCl8gDEag_t",
        "outputId": "dd4265e6-2038-47a4-d904-af5f58060fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.2024.wikigiga.300d.zip\n",
            "  inflating: wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip glove.2024.wikigiga.300d.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7CnkwWvcN6_",
        "outputId": "dd5fa399-5af8-4a74-c80b-703f3fca35c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "file:  wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt\n",
            "   1: the -0.15277 -0.092999 -0.229052 -0.447638 0.363178 0.10623300000000001 -0.139482 -0.10426900000000001 -0.00283300000000...\n",
            "   2: , -0.75903 0.008666999999999994 0.143945 0.254658 -0.016168 0.04800499999999999 -0.05103200000000001 0.129723 0.236888 -...\n",
            "   3: . -0.147488 0.068442 0.433377 -0.199067 -0.342036 0.164133 0.137323 -0.07649799999999998 0.057161000000000003 -0.1137419...\n",
            "\n",
            "file:  GoogleNews_300d.txt\n",
            "   1: </s> 0.0011291504 -0.00089645386 0.00031852722 0.0015335083 0.0011062622 -0.0014038086 -3.0517578e-05 -0.0004196167 -0.0...\n",
            "   2: in 0.0703125 0.08691406 0.087890625 0.0625 0.06933594 -0.10888672 -0.08154297 -0.15429688 0.020751953 0.13183594 -0.1137...\n",
            "   3: for -0.011779785 -0.04736328 0.044677734 0.06347656 -0.018188477 -0.063964844 -0.0013122559 -0.072265625 0.064453125 0.0...\n",
            "\n",
            "file:  wiki-news-300d-1M.txt\n",
            "   1: , 0.1073 0.0089 0.0006 0.0055 -0.0646 -0.0600 0.0450 -0.0133 -0.0357 0.0430 -0.0356 -0.0032 0.0073 -0.0001 0.0258 -0.016...\n",
            "   2: the 0.0897 0.0160 -0.0571 0.0405 -0.0696 -0.1237 0.0301 0.0248 -0.0303 0.0174 0.0063 0.0184 0.0217 -0.0257 0.0350 -0.024...\n",
            "   3: . 0.0004 0.0032 -0.0204 0.0479 -0.0450 -0.1165 0.0142 0.0068 -0.0334 -0.0504 0.0224 -0.0029 -0.0258 0.0265 0.0059 -0.045...\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "files = [\n",
        "    Path(\"/content/wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt\"),\n",
        "    Path(\"GoogleNews_300d.txt\"),\n",
        "    Path(\"wiki-news-300d-1M.txt\"),\n",
        "]\n",
        "\n",
        "for f in files:\n",
        "    print(f\"\\nfile: \", f.name)\n",
        "    try:\n",
        "        with f.open(\"r\", encoding=\"utf8\") as fh:\n",
        "            for i in range(3):\n",
        "                print(f\"  {i+1:>2}: {fh.readline().strip()[:120]}...\")\n",
        "    except Exception as e:\n",
        "        print(\"Error :\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import gzip\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "RDbG10RQu004"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\n",
        "    (\"/content/wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt\", \"glove_300D\"),\n",
        "    (\"/content/wiki-news-300d-1M.txt\", \"FastText_300D\"),\n",
        "    (\"/content/GoogleNews_300d.txt\", \"Word2Vec_300D\")\n",
        "]"
      ],
      "metadata": {
        "id": "fJTcWgfYt8Ue"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "def save_and_compress_outputs(data, path: Path):\n",
        "    \"\"\"Save JSON â†’ directly compress to outputs folder.\"\"\"\n",
        "    gzip_path = path.with_suffix(path.suffix + \".gz\")\n",
        "\n",
        "    # Compress JSON directly into gzip file\n",
        "    with gzip.open(gzip_path, \"wt\", encoding=\"utf8\") as f_out:\n",
        "        json.dump(data, f_out, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"ğŸ—œï¸ Compressed â†’ {gzip_path.relative_to(BASE_OUTPUT_DIR.parent)}\")\n",
        "\n",
        "def build_vis_json(words, reduced, edges, cluster_labels):\n",
        "    return [\n",
        "        {\"word\": w, \"x\": float(x), \"y\": float(y), \"z\": float(z),\n",
        "         \"cluster\": int(c), \"edges\": edges[i]}\n",
        "        for i, (w, (x, y, z), c) in enumerate(zip(words, reduced, cluster_labels))\n",
        "    ]"
      ],
      "metadata": {
        "id": "qx9PlxcuveKe"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edge_params(word_count):\n",
        "    \"\"\"Get edge parameters based on dataset size\"\"\"\n",
        "    if word_count >= 10000:\n",
        "        k_neighbors = 3\n",
        "    elif word_count >= 5000:\n",
        "        k_neighbors = 4\n",
        "    else:\n",
        "        k_neighbors = 5\n",
        "\n",
        "    return {\"k_neighbors\": k_neighbors}"
      ],
      "metadata": {
        "id": "tiSbozY2yL3a"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clustering_params(word_count, algorithm=\"kmeans\"):\n",
        "    \"\"\"Get clustering parameters based on dataset size\"\"\"\n",
        "    if algorithm == \"kmeans\":\n",
        "        n_clusters = max(8, min(50, word_count // 150))\n",
        "        return {\"n_clusters\": n_clusters, \"random_state\": 42, \"n_init\": 10}\n",
        "    elif algorithm == \"dbscan\":\n",
        "        # Tighter eps for larger datasets to avoid giant clusters\n",
        "        if word_count >= 10000:\n",
        "            eps = 0.25\n",
        "            min_samples = 3\n",
        "        elif word_count >= 5000:\n",
        "            eps = 0.28\n",
        "            min_samples = 2\n",
        "        else:\n",
        "            eps = 0.3\n",
        "            min_samples = 2\n",
        "        return {\"eps\": eps, \"min_samples\": min_samples}\n",
        "    return {}"
      ],
      "metadata": {
        "id": "iBD6yp4yyh0i"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline"
      ],
      "metadata": {
        "id": "_9py-CIg1heD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding_file(embed_path: Path, top_n: int):\n",
        "    \"\"\"Load top-N word embeddings from text file.\"\"\"\n",
        "    words, vectors = [], []\n",
        "    with open(embed_path, \"r\", encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 10:\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=float)\n",
        "            except ValueError:\n",
        "                continue\n",
        "            words.append(word)\n",
        "            vectors.append(vec)\n",
        "            if len(words) >= top_n:\n",
        "                break\n",
        "    if not vectors:\n",
        "        raise RuntimeError(f\"No valid vectors found in {embed_path}\")\n",
        "    vectors = np.vstack(vectors)\n",
        "    print(f\"âœ… Loaded {len(words)} words, {vectors.shape[1]}D vectors\")\n",
        "    return words, vectors"
      ],
      "metadata": {
        "id": "qo5VG8kl1gg_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "\n",
        "def reduce_embeddings(vectors, reducer=\"pca\"):\n",
        "    \"\"\"Reduce high-dimensional vectors to 3D using PCA or UMAP.\"\"\"\n",
        "    print(f\"ğŸ”¹ Reducing with {reducer.upper()} â†’ 3D...\")\n",
        "    if reducer == \"pca\":\n",
        "        model = PCA(n_components=3)\n",
        "    elif reducer == \"umap\":\n",
        "        model = UMAP(n_components=3, random_state=42,n_neighbors=15, min_dist=0.1)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown reducer: {reducer}\")\n",
        "    reduced = model.fit_transform(vectors)\n",
        "    print(f\"âœ… Reduced to shape: {reduced.shape}\")\n",
        "    return reduced"
      ],
      "metadata": {
        "id": "FxWxwBdI2Dqf"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering\n",
        "\n",
        "def cluster(reduced, top_n, algo=\"kmeans\"):\n",
        "    \"\"\"Cluster reduced embeddings.\"\"\"\n",
        "    print(\"ğŸ”¹ Clustering points...\")\n",
        "    if algo == \"dbscan\":\n",
        "        params = get_clustering_params(top_n, \"dbscan\")\n",
        "        clusterer = DBSCAN(**params)\n",
        "    else:\n",
        "        params = get_clustering_params(top_n, \"kmeans\")\n",
        "        clusterer = KMeans(**params)\n",
        "    cluster_labels = clusterer.fit_predict(reduced)\n",
        "\n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_noise = list(cluster_labels).count(-1)\n",
        "    print(f\"âœ… {n_clusters} clusters, {n_noise} noise points\")\n",
        "    return cluster_labels"
      ],
      "metadata": {
        "id": "BPU8Tr2h3baL"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K Nearest Neighbors\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def build_edges(reduced, cluster_labels, top_n):\n",
        "    \"\"\"Build k-nearest-neighbor edges inside each cluster.\"\"\"\n",
        "    print(\"ğŸ”¹ Computing edges...\")\n",
        "    edge_params = get_edge_params(top_n)\n",
        "    k_neighbors = edge_params[\"k_neighbors\"]\n",
        "\n",
        "    edges_by_point = [[] for _ in range(len(reduced))]\n",
        "    cluster_groups = {}\n",
        "    for idx, cid in enumerate(cluster_labels):\n",
        "        if cid == -1:\n",
        "            continue\n",
        "        cluster_groups.setdefault(cid, []).append(idx)\n",
        "\n",
        "    total_edges = 0\n",
        "    for cid, point_indices in cluster_groups.items():\n",
        "        if len(point_indices) < 2:\n",
        "            continue\n",
        "        cluster_points = reduced[point_indices]\n",
        "        k = min(k_neighbors, len(point_indices) - 1)\n",
        "        if k < 1:\n",
        "            continue\n",
        "        nn = NearestNeighbors(n_neighbors=k + 1)\n",
        "        nn.fit(cluster_points)\n",
        "        _, neighbor_indices = nn.kneighbors(cluster_points)\n",
        "        for i, global_idx in enumerate(point_indices):\n",
        "            for j in range(1, k + 1):\n",
        "                neighbor_global_idx = point_indices[neighbor_indices[i][j]]\n",
        "                if neighbor_global_idx not in edges_by_point[global_idx]:\n",
        "                    edges_by_point[global_idx].append(neighbor_global_idx)\n",
        "                    total_edges += 1\n",
        "    print(f\"âœ… Built {total_edges} edges (â‰¤{k_neighbors} per point)\")\n",
        "    return edges_by_point, total_edges"
      ],
      "metadata": {
        "id": "lC5IrsgU3xiR"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_OUTPUT_DIR = Path(\"outputs\")\n",
        "BASE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def process_embedding_file(model_name, path: Path, top_ns=(1000, 5000)):\n",
        "    \"\"\"\n",
        "    Process one embedding file (Loading -> DR -> Clustering -> Saving)\n",
        "    \"\"\"\n",
        "    output_dir = BASE_OUTPUT_DIR / model_name\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    words, vectors = load_embedding_file(path, max(top_ns))\n",
        "\n",
        "    full_data = {w: vec.tolist() for w, vec in zip(words, vectors)}\n",
        "    save_and_compress_outputs(full_data, output_dir / f\"{model_name}_full.json\")\n",
        "\n",
        "    for top_n in top_ns:\n",
        "        subset_words = words[:top_n]\n",
        "        subset_vecs = vectors[:top_n]\n",
        "\n",
        "        for method in [\"pca\", \"umap\"]:\n",
        "            reduced = reduce_embeddings(subset_vecs, method)\n",
        "            cluster_labels = cluster(reduced, top_n)\n",
        "            edges, total_edges = build_edges(reduced, cluster_labels, top_n)\n",
        "            vis_data = build_vis_json(subset_words, reduced, edges, cluster_labels)\n",
        "            save_and_compress_outputs(\n",
        "                vis_data, output_dir / f\"{model_name}_{top_n}_{method}_3d.json\"\n",
        "            )\n",
        "\n",
        "    print(f\"âœ… Finished processing {model_name}\\n\")"
      ],
      "metadata": {
        "id": "7Oci6e9774NP"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for path, name in filenames:\n",
        "        process_embedding_file(name, Path(path), top_ns=(1000, 5000, 10000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaK92kL59RqF",
        "outputId": "b479ab6e-0d83-43a9-f12a-69908440aad1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 10000 words, 300D vectors\n",
            "ğŸ’¾ Saved â†’ glove_300D_full.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_full.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_1000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_1000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_1000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_1000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_5000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_5000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_5000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_5000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 30000 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_10000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_10000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 30000 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ glove_300D_10000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/glove_300D/glove_300D_10000_umap_3d.json.gz\n",
            "âœ… Finished processing glove_300D\n",
            "\n",
            "âœ… Loaded 10000 words, 300D vectors\n",
            "ğŸ’¾ Saved â†’ FastText_300D_full.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_full.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_1000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_1000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_1000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_1000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_5000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_5000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_5000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_5000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 30000 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_10000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_10000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 30000 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ FastText_300D_10000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/FastText_300D/FastText_300D_10000_umap_3d.json.gz\n",
            "âœ… Finished processing FastText_300D\n",
            "\n",
            "âœ… Loaded 10000 words, 300D vectors\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_full.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_full.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_1000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_1000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (1000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 8 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 5000 edges (â‰¤5 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_1000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_1000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_5000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_5000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (5000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 33 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 20000 edges (â‰¤4 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_5000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_5000_umap_3d.json.gz\n",
            "ğŸ”¹ Reducing with PCA â†’ 3D...\n",
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 29996 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_10000_pca_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_10000_pca_3d.json.gz\n",
            "ğŸ”¹ Reducing with UMAP â†’ 3D...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reduced to shape: (10000, 3)\n",
            "ğŸ”¹ Clustering points...\n",
            "âœ… 50 clusters, 0 noise points\n",
            "ğŸ”¹ Computing edges...\n",
            "âœ… Built 30000 edges (â‰¤3 per point)\n",
            "ğŸ’¾ Saved â†’ Word2Vec_300D_10000_umap_3d.json\n",
            "ğŸ—œï¸ Compressed â†’ outputs/Word2Vec_300D/Word2Vec_300D_10000_umap_3d.json.gz\n",
            "âœ… Finished processing Word2Vec_300D\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional (For colab)"
      ],
      "metadata": {
        "id": "QIznM-cACRIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "shutil.make_archive(\"outputs\", \"zip\", \"outputs\")\n",
        "files.download(\"outputs.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SDms0KuQCX7U",
        "outputId": "a4ab71fe-34b1-48d6-db63-1a02ac2c745a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_97a7244b-e175-401c-9651-2daac1feec72\", \"outputs.zip\", 33500783)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}